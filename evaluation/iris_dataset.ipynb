{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "\n",
    "# function from sklearn to load IRIS data\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# data science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for outlier detection\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.knn import KNN\n",
    "from outlier_detection.rocf import ROCF\n",
    "from outlier_detection.cbof import CBOF\n",
    "\n",
    "# preprocessing\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# for evaluation\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# exploratory data analysis\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# bayesian hyperparameter optimization\n",
    "from hyperopt import hp, Trials, fmin, tpe, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "from time import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRIS Dataset\n",
    "\n",
    "The IRIS dataset consists of 3 different types of irises’ (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray. (Source: https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html).\n",
    "\n",
    "The authors in the paper \"A novel outlier cluster detection algorithm without top-n parameter\" selected 50 examples from “setosa” class as normal cluster (labelled as 0) and respectively selected 10 objects from “versicolor” and “virginica” classes as two small outlier clusters (labelled as 1).\n",
    "\n",
    "The input data has 4 dimensions:\n",
    "1. sepal length (cm)\n",
    "2. sepal width (cm)\n",
    "3. petal length (cm)\n",
    "4. petal width (cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    iris_od = pd.read_csv(\"../data/iris.csv\")\n",
    "except:\n",
    "    # load iris data\n",
    "    X_iris, y_iris = load_iris(return_X_y=True, as_frame=True)\n",
    "    iris_data = X_iris.copy()\n",
    "    iris_data['target'] = y_iris\n",
    "\n",
    "    # extract normal data\n",
    "    iris_normal = iris_data.loc[iris_data['target'] == 0]\n",
    "\n",
    "    # extract anomaly data\n",
    "    iris_anomaly = pd.DataFrame()\n",
    "\n",
    "    for target in [1, 2]:\n",
    "        # retrieve class and randomly select 10 objects\n",
    "        iris_class = iris_data.loc[iris_data['target'] == target].sample(10, random_state=1234)\n",
    "        # append to anomaly dataframe\n",
    "        iris_anomaly = iris_anomaly.append(iris_class, ignore_index=True)\n",
    "\n",
    "    # change all anomaly targets to '1'\n",
    "    iris_anomaly['target'] = 1\n",
    "\n",
    "    # combine normal and outlier dataset\n",
    "    iris_od = iris_normal.append(iris_anomaly, ignore_index=True)\n",
    "    iris_od = iris_od.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # save csv as randomness in sampling may produce different results\n",
    "    iris_od.to_csv(\"../data/iris.csv\", index=False)\n",
    "\n",
    "# retrieve X, y\n",
    "X_iris = iris_od.iloc[:, 0:4].values\n",
    "y_iris = iris_od.target\n",
    "\n",
    "print(\"Shape of data: \", iris_od.shape)\n",
    "iris_od.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.tight_layout(pad=2.0)\n",
    "\n",
    "i = 0\n",
    "for row in range(2):\n",
    "    for col in range(2):\n",
    "        # plot histogram distributions of features\n",
    "        axs[row, col].hist(iris_od[iris_od.columns[i]], color='darkgray')\n",
    "        axs[row, col].set_title(iris_od.columns[i])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_od.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicate Parameters Suggested in Paper\n",
    "\n",
    "The paper attempted the following algorithms and parameters:\n",
    "1. LOF\n",
    "    - k = 20 (number of neighbors to use for kneighbors queries)\n",
    "    - n = 10, 20, 30 (number of outliers)\n",
    "2. CBOF\n",
    "    - k = 8\n",
    "    - alpha = 0.90, 0.80, 0.68\n",
    "    - \"if alpha is set to 90%, we intend to regard clusters which contain 90% of data points as normal clusters, and the others are abnormal clusters.\"\n",
    "3. ROCF\n",
    "    - k = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model functions\n",
    "n_samples = len(iris_od)\n",
    "\n",
    "# dictionary of models & parameters to test\n",
    "# LOF, CBOF, ROCF are replications of Table 4 of the paper\n",
    "functions_dict = {\n",
    "    'LOF (k=20, n=10)': { 'algo': 'LOF', 'k':20, 'n':10, 'f':LOF(n_neighbors=20, contamination=10/n_samples) }, \n",
    "    'LOF (k=20, n=20)': { 'algo': 'LOF', 'k':20, 'n':20, 'f':LOF(n_neighbors=20, contamination=20/n_samples) },\n",
    "    'LOF (k=20, n=30)': { 'algo': 'LOF', 'k':20, 'n':30, 'f':LOF(n_neighbors=20, contamination=30/n_samples) },\n",
    "    'CBOF (k=8, alpha=0.9)': {'algo': 'CBOF', 'k': 8, 'n':7, 'f':CBOF(k=8, contamination=0.1) },\n",
    "    'CBOF (k=8, alpha=0.8)': {'algo': 'CBOF', 'k': 8, 'n':14, 'f':CBOF(k=8, contamination=0.2) },\n",
    "    'CBOF (k=8, alpha=0.68)': {'algo': 'CBOF', 'k': 8, 'n':23, 'f':CBOF(k=8, contamination=0.32) },\n",
    "    'ROCF (k=9)': { 'algo': 'ROCF', 'k':9, 'n':None, 'f':ROCF(distance_metric=\"euclidean\", k=9) }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output dataframe\n",
    "iris_results = pd.DataFrame(columns=['algo', 'k', 'n', 'outlier_rate', 'recall', 'precision', 'f1'])\n",
    "\n",
    "for name, f_dict in functions_dict.items():\n",
    "    # initialise classifier\n",
    "    clf = f_dict['f']\n",
    "\n",
    "    # fit classifier on data\n",
    "    clf.fit(X_iris)\n",
    "\n",
    "    # retrieve outliers on train set\n",
    "    try:\n",
    "        y_pred = clf.get_outliers()\n",
    "    except:\n",
    "        y_pred = clf.labels_\n",
    "\n",
    "\n",
    "    # derive evaluation metrics\n",
    "    report = classification_report(y_true=y_iris, y_pred=y_pred, output_dict=True)['1']\n",
    "\n",
    "    row = { \n",
    "        'algo': f_dict['algo'], 'k': f_dict['k'], 'n': f_dict['n'],\n",
    "        'precision': report['precision'], 'recall': report['recall'], 'f1': report['f1-score']\n",
    "    }\n",
    "\n",
    "    # retrieve outlier rate\n",
    "    try:\n",
    "        row['outlier_rate'] = clf.get_outlier_rate()\n",
    "    except:\n",
    "        row['outlier_rate'] = clf.contamination\n",
    "\n",
    "    iris_results = iris_results.append(row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "iris_results.sort_values(by=['f1'], ascending=False)\n",
    "iris_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe from the table above that the results we obtained are not consistent with those in the paper. This could be due to reasons like (1) different subset of data (anomalies are randomly sampled), (2) different methods of preprocessing, (3) different methods of feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Besides the 3 methods compared in the paper (LOF, CBOF and ROCF), we also tested other outlier detection algorithms, namely K-Nearest Neighbors, Isolation Forest, and One-Class SVM.\n",
    "\n",
    "For these 6 methods, we conducted an in-sample hyperparameter tuning, to find the optimal parameters for each of the outlier detection algorithms for our IRIS dataset. We used Bayesian hyperparameter optimisation, which uses Bayes Theorem to direct the hyperparameter search in order to find the minimum or maximum of an objective function.\n",
    "\n",
    "The contamination factor was set at 20/70 for all algorithms, which is the true contamination factor.\n",
    "\n",
    "In a later section of this notebook, we will show how the lack of knowledge on the contamination factor will affect the outlier detection results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Hyperparameter Optimisation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt(param_space, X, y, num_eval, classifier):  \n",
    "    '''\n",
    "    Function that performs Bayesian hyperparameter optimisation \n",
    "    to find the optimal parameters for the outlier detection algorithm.\n",
    "    \n",
    "    Inputs:\n",
    "        param_space (dict): A dictionary of the parameters and corresponding space to search.\n",
    "        X (array): Features of the dataset.\n",
    "        y (array): Labels of the dataset (0 = normal; 1 = anomaly).\n",
    "        \n",
    "        num_eval (int): Number of evaluation rounds.\n",
    "        classifier (pyOD Object): Outlier detection algorithm.\n",
    "        \n",
    "    Outputs:\n",
    "        trials\n",
    "        -min(loss) (float): Best in-sample F1 score.\n",
    "        best_param_values (dict): Dictionary of the best parameters for the classifier.\n",
    "    '''\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    def objective_function(params):\n",
    "        # initialise classifier\n",
    "        clf = classifier(**params)\n",
    "        # fit data\n",
    "        clf.fit(X)\n",
    "        # predict\n",
    "        try:\n",
    "            y_pred = clf.labels_\n",
    "        except: # ROCF algorithm\n",
    "            y_pred = clf.get_outliers()\n",
    "        # get F1 score\n",
    "        score = f1_score(y_true=y, y_pred=y_pred)\n",
    "        report = classification_report(y_true=y, y_pred=y_pred, output_dict=True)['1']\n",
    "        # objective is to maximize F1 i.e. minimize -F1\n",
    "        return {'loss': -report['f1-score'], 'status': STATUS_OK, 'precision': report['precision'], \n",
    "               'recall': report['recall']}\n",
    "    \n",
    "    trials = Trials()\n",
    "    \n",
    "    # minimise objective function\n",
    "    best_param = fmin(objective_function, param_space, algo=tpe.suggest, max_evals=num_eval, \n",
    "                      trials=trials, rstate= np.random.RandomState(1))\n",
    "    \n",
    "    loss = [x['result']['loss'] for x in trials.trials] \n",
    "    precision = [x['result']['precision'] for x in trials.trials] \n",
    "    recall = [x['result']['recall'] for x in trials.trials] \n",
    "    \n",
    "    best_ind = loss.index(min(loss))\n",
    "    \n",
    "    best_param_values = best_param\n",
    "    \n",
    "    return trials, -loss[best_ind], best_param_values, precision[best_ind], recall[best_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict to store hyperopt inputs for each algorithm\n",
    "hyperopt_inputs = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Outlier Factor (LOF)\n",
    "\n",
    "Reference: https://pyod.readthedocs.io/en/latest/pyod.models.html?#module-pyod.models.lof\n",
    "\n",
    "Parameters:\n",
    "1. `n_neighbors` (default=20): Number of neighbors to use by default for kneighbors queries.\n",
    "2. `algorithm` (default='auto'): Algorithm used to compute the nearest neighbors.\n",
    "3. `leaf_size` (default=30): Leaf size passed to BallTree or KDTree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameter search range\n",
    "LOF_param_hyperopt = {\n",
    "    'n_neighbors': scope.int(hp.quniform('n_neighbors', 8, 25, 1)), # might want to decrease upper limit\n",
    "    'algorithm': hp.choice('algorithm', ['ball_tree', 'kd_tree', 'brute']),\n",
    "    'leaf_size': scope.int(hp.quniform('leaf_size', 5, 15, 1)),\n",
    "    'contamination': 20/70, # set to actual outlier % \n",
    "}\n",
    "\n",
    "# num_eval proportional to number of combinations of parameter values for different models\n",
    "# num_eval = 3**(num_params_to_tune)\n",
    "LOF_inputs = {'classifier': LOF, 'param_space': LOF_param_hyperopt, 'num_eval': 3**3}\n",
    "hyperopt_inputs['LOF'] = LOF_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster-Based Outlier Factor (CBOF)\n",
    "\n",
    "Reference: https://github.com/ValaryLim/pyODPlus/blob/main/outlier_detection/cbof.py\n",
    "\n",
    "Parameters:\n",
    "1. `k` (default=5): k number of nearest neigbours to compute Local Outlier Factor and Local Reachability Density.\n",
    "2. `lofub` (default=2.0): Threshold set for any point to be considered a core point in a cluster. If LOF(p) <= lofub, p is a core point. \n",
    "3. `pct` (default=0.5): Value in range (0, 1]. Percentage to consider if points are local density reachable from one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameter search range\n",
    "CBOF_param_hyperopt = {\n",
    "    'k': scope.int(hp.quniform('n_neighbors', 8, 25, 1)),\n",
    "    'lofub': hp.uniform('lofub', 0.5, 3.0),\n",
    "    'pct': hp.uniform('pct', 0.2, 0.8),\n",
    "    'contamination': 20/70, # set to actual outlier % \n",
    "}\n",
    "\n",
    "CBOF_inputs = {'classifier': CBOF, 'param_space': CBOF_param_hyperopt, 'num_eval': 3**3}\n",
    "hyperopt_inputs['CBOF'] = CBOF_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Outlier Cluster Factor (ROCF)\n",
    "\n",
    "Reference: https://github.com/ValaryLim/pyODPlus/blob/main/outlier_detection/rocf.py\n",
    "\n",
    "Parameters:\n",
    "1. `k` (default=3): k number of nearest neigbours used to form MUtual Neighbour Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROCF_param_hyperopt = {\n",
    "    'k': scope.int(hp.quniform('n_neighbors', 15, 25, 1)),\n",
    "}\n",
    "\n",
    "ROCF_inputs = {'classifier': ROCF, 'param_space': ROCF_param_hyperopt, 'num_eval': 3**1}\n",
    "hyperopt_inputs['ROCF'] = ROCF_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors\n",
    "\n",
    "Reference: https://pyod.readthedocs.io/en/latest/pyod.models.html?#module-pyod.models.knn\n",
    "\n",
    "Parameters:\n",
    "1. `n_neighbors` (default=5): Number of neighbors to use by default for k neighbors queries.\n",
    "2. `method` (default='largest'): `largest` uses the distance to the kth neighbor as the outlier score, `mean` uses the average of all k neighbors as the outlier score, `median` uses the median of the distance to k neighbors as the outlier score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_param_hyperopt = {\n",
    "    'contamination': 20/70,\n",
    "    'n_neighbors': scope.int(hp.quniform('n_neighbors', 8, 25, 1)),\n",
    "    'method': hp.choice('method', ['largest', 'mean', 'median']),\n",
    "}\n",
    "\n",
    "KNN_inputs = {'classifier': KNN, 'param_space': KNN_param_hyperopt, 'num_eval': 3**2}\n",
    "hyperopt_inputs['KNN'] = KNN_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest (IForest)\n",
    "\n",
    "Reference: https://pyod.readthedocs.io/en/latest/pyod.models.html?#module-pyod.models.iforest\n",
    "\n",
    "Parameters:\n",
    "1. `n_estimators` (default=100): The number of base estimators in the ensemble.\n",
    "2. `max_samples` (default='auto' which is min(256, n_samples)): The number of samples to draw from X to train each base estimator.\n",
    "3. `max_features` (default=1): The number of features to draw from X to train each base estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IF_param_hyperopt = {\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 1, 50, 1)),\n",
    "    'max_samples': scope.int(hp.quniform('max_samples', 10, 35, 1)),    \n",
    "    'contamination': 20/70,\n",
    "    'max_features': scope.int(hp.quniform('max_features', 1, 4, 1)),    \n",
    "}\n",
    "\n",
    "IF_inputs = {'classifier': IForest, 'param_space': IF_param_hyperopt, 'num_eval': 3**3}\n",
    "hyperopt_inputs['IForest'] = IF_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Class Support Vector Machine (OCSVM)\n",
    "\n",
    "Reference: https://pyod.readthedocs.io/en/latest/pyod.models.html?#module-pyod.models.ocsvm\n",
    "\n",
    "Parameters:\n",
    "1. `kernel` (default='rbf'): Specifies the kernel type to be used in the algorithm. \n",
    "2. `nu` (default=0.5):  An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OCSVM_param_hyperopt = {\n",
    "    'kernel': hp.choice('kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'nu': hp.uniform('nu', 0.4, 0.8),\n",
    "    'contamination': 20/70,\n",
    "}\n",
    "\n",
    "OCSVM_inputs = {'classifier': OCSVM, 'param_space': OCSVM_param_hyperopt, 'num_eval': 3**2}\n",
    "hyperopt_inputs['OCSVM'] = OCSVM_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_results_tuned = pd.DataFrame(columns=['algo', 'recall', 'precision', 'f1'])\n",
    "\n",
    "for algo, algo_inputs in hyperopt_inputs.items():\n",
    "    # run hyperopt\n",
    "    algo_hyperopt = hyperopt(algo_inputs['param_space'], \\\n",
    "                             X_iris, y_iris, \\\n",
    "                             algo_inputs['num_eval'], algo_inputs['classifier'])\n",
    "    # retrieve best parameters\n",
    "    algo_opt = algo_hyperopt[2]\n",
    "    algo_opt['f1'] = algo_hyperopt[1] # add f1 score\n",
    "    algo_opt['precision'] = algo_hyperopt[3]\n",
    "    algo_opt['recall'] = algo_hyperopt[4]\n",
    "    algo_opt['algo'] = algo # add algo name\n",
    "    # add to results dataframe\n",
    "    iris_results_tuned = iris_results_tuned.append(algo_opt, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_results_tuned.sort_values(by=['f1'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_results_tuned[['algo', 'recall', 'precision', 'f1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, we observe that with the right contamination factor of 20/70 set in the algorithms, IForest, OCSVM, and KNN were able to perfectly identify all the outliers with tuning of the other hyperparameters.\n",
    "\n",
    "ROCF, which does not require the top-n parameter, also managed to identify all outliers perfectly with a n-neighbors of 22."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of \"Top-n\" parameter\n",
    "\n",
    "In the above analysis, we have fixed the outlier rate to 20/70=0.2857 as we know the outlier rate in the IRIS dataset sampled. In this section, we will demonstrate how different values of outlier rate (pre-specified) will affect LOF, CBOF, KNN, IForest, and OCSVM.\n",
    "\n",
    "This supports the key point of the paper - that the lack of knowledge on the outlier rate will affect the performance of the algorithm hence, the ability to detect outliers in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_results = pd.DataFrame(columns=['algo', 'outlier_rate', 'recall', 'precision', 'f1'])\n",
    "\n",
    "outlier_rate_lst = [10/70, 20/70, 30/70]\n",
    "algos_dict = {'LOF': LOF, 'CBOF': CBOF, 'ROCF': ROCF, 'KNN': KNN, 'IForest': IForest, 'OCSVM': OCSVM}\n",
    "\n",
    "# for each algorithm\n",
    "for algo_name, f in algos_dict.items():\n",
    "    \n",
    "    # for each outlier rate\n",
    "    for outlier_rate in outlier_rate_lst:\n",
    "        \n",
    "        outlier_rate_add = outlier_rate\n",
    "        \n",
    "        # get best parameters from tuning\n",
    "        algo_params = iris_results_tuned.loc[iris_results_tuned['algo'] == algo_name].reset_index().iloc[0]\n",
    "        \n",
    "        # initialise classifier\n",
    "        if algo_name == 'LOF':\n",
    "            algo_lst = ['ball_tree', 'kd_tree', 'brute']\n",
    "            clf = LOF(algorithm=algo_lst[int(algo_params['algorithm'])], contamination=outlier_rate,\n",
    "                      leaf_size=algo_params['leaf_size'], n_neighbors=int(algo_params['n_neighbors']))\n",
    "            \n",
    "        elif algo_name == 'KNN':\n",
    "            method_lst = ['largest', 'mean', 'median']\n",
    "            clf = KNN(n_neighbors=int(algo_params['n_neighbors']), contamination=outlier_rate, \\\n",
    "                      method=method_lst[int(algo_params['method'])])\n",
    "            \n",
    "        elif algo_name == 'IForest':\n",
    "            clf = IForest(max_features=int(algo_params['max_features']), contamination=outlier_rate, \\\n",
    "                          max_samples=int(algo_params['max_samples']), n_estimators=int(algo_params['n_estimators']))\n",
    "            \n",
    "        elif algo_name == 'CBOF':\n",
    "            clf = CBOF(k=int(algo_params['n_neighbors']), contamination=outlier_rate, \\\n",
    "                       lofub=algo_params['lofub'], pct=algo_params['pct'])\n",
    "            \n",
    "        elif algo_name == 'OCSVM':\n",
    "            kernel_lst = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "            clf = OCSVM(kernel=kernel_lst[int(algo_params['kernel'])], \\\n",
    "                        nu=algo_params['nu'], contamination=outlier_rate)\n",
    "        \n",
    "        elif algo_name == 'ROCF':\n",
    "            if outlier_rate != 20/70:\n",
    "                continue\n",
    "            else:\n",
    "                clf = ROCF(k=int(algo_params['n_neighbors']))\n",
    "            \n",
    "        # fit classifier on data\n",
    "        clf.fit(X_iris)\n",
    "        \n",
    "        # retrieve predictions\n",
    "        try:\n",
    "            y_pred = clf.get_outliers()\n",
    "        except:\n",
    "            y_pred = clf.labels_\n",
    "            \n",
    "        if algo_name == 'ROCF':\n",
    "            outlier_rate_add = clf.get_outlier_rate() # get outlier rate calculated by rocf algorithm\n",
    "        \n",
    "        report = classification_report(y_true=y_iris, y_pred=y_pred, output_dict=True)\n",
    "        f1 = report['1']['f1-score']\n",
    "        precision = report['1']['precision']\n",
    "        recall = report['1']['recall']\n",
    "        top_n_results = top_n_results.append({'algo': algo_name, 'outlier_rate': outlier_rate_add, 'recall': recall, \\\n",
    "                                              'precision': precision, 'f1': f1}, \\\n",
    "                                             ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, we observe that generally, if our estimate of outlier rate is inaccurate, running the outlier detection algorithms will likely lead to sub-optimal outlier detection. This highlights the usefulness of the ROCF algorithm which does not require this top-n parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of 0.1 ROCF Limit\n",
    "\n",
    "- Authors in the paper claimed that the best threshold (threshold set for any cluster to be considered an outlier) is fixed at 0.1.\n",
    "- Each cluster has an ROCF value. If max({ROCF}) < threshold, no cluster is considered as outlier.\n",
    "- Else, all clusters with smaller size than cluster with max ROCF are tagged as outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameter search range\n",
    "n_neighbors = 22 # optimal value found from hyperparameter tuning\n",
    "threshold_lst = [x for x in np.arange(0.05, 0.31, 0.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output dataframe\n",
    "iris_rocf_results = pd.DataFrame(columns=['threshold', 'outlier_rate', 'recall', 'precision', 'f1'])\n",
    "\n",
    "for threshold in threshold_lst:\n",
    "    # create ROCF classifier\n",
    "    clf = ROCF(k=n_neighbors, threshold=threshold)\n",
    "\n",
    "    # fit classifier on data\n",
    "    clf.fit(X_iris)\n",
    "\n",
    "    # retrieve predictions\n",
    "    y_pred = clf.get_outliers()\n",
    "\n",
    "    # derive evaluation metrics\n",
    "    report = classification_report(y_true=y_iris, y_pred=y_pred, output_dict=True)\n",
    "    f1 = report['1']['f1-score']\n",
    "    precision = report['1']['precision']\n",
    "    recall = report['1']['recall']\n",
    "\n",
    "    # retrieve outlier rate\n",
    "    outlier_rate = clf.get_outlier_rate()\n",
    "\n",
    "    iris_rocf_results = iris_rocf_results.append({'threshold': threshold, 'outlier_rate': outlier_rate, \\\n",
    "                                                  'recall': recall, 'precision': precision, 'f1': f1}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original dataset, without preprocessing\n",
    "iris_rocf_results.sort_values(by=['f1'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that with an optimal value for k, the threshold does not really affect the classification results in the threshold range of 0.05 to 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max rocf with best k\n",
    "clf = ROCF(k=22)\n",
    "clf.fit(X_iris)\n",
    "print(max(clf.get_rocfs()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specification of Parameter k\n",
    "Show that specification of parameter k will affect results of ROCF outlier detection, and this parameter is not easy to determine as well (limitation of ROCF, even though ROCF does not require top-n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameter search range\n",
    "n_neighbors_lst = [x for x in range(1, 31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output dataframe\n",
    "iris_rocf_results_k = pd.DataFrame(columns=['k', 'outlier_rate', 'recall', 'precision', 'f1'])\n",
    "\n",
    "for n in n_neighbors_lst:\n",
    "    # create ROCF classifier\n",
    "    clf = ROCF(k=n)\n",
    "\n",
    "    # fit classifier on data\n",
    "    clf.fit(X_iris)\n",
    "\n",
    "    # retrieve predictions\n",
    "    y_pred = clf.get_outliers()\n",
    "\n",
    "    # derive evaluation metrics\n",
    "    report = classification_report(y_true=y_iris, y_pred=y_pred, output_dict=True)\n",
    "    f1 = report['1']['f1-score']\n",
    "    precision = report['1']['precision']\n",
    "    recall = report['1']['recall']\n",
    "\n",
    "    # retrieve outlier rate\n",
    "    outlier_rate = clf.get_outlier_rate()\n",
    "\n",
    "    iris_rocf_results_k = iris_rocf_results_k.append({'k': n, 'outlier_rate': outlier_rate, \\\n",
    "                                                      'recall': recall, 'precision': precision, 'f1': f1}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_rocf_results_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot graph of F1 score against hyperparameter k\n",
    "plt.scatter(iris_rocf_results_k['k'], iris_rocf_results_k['f1'], marker='.', color='darkblue')\n",
    "plt.title('F1 Score against k (IRIS dataset)')\n",
    "plt.xlabel('k, number of nearest neighbors')\n",
    "plt.ylabel('F1 Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that the value of k matters in terms of classification performance. F1 score changes depending on the value for k, given all other factors are constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

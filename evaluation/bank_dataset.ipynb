{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "\n",
    "# data science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for outlier detection\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.knn import KNN\n",
    "from outlier_detection.rocf import ROCF\n",
    "from outlier_detection.cbof import CBOF\n",
    "\n",
    "# preprocessing\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# for evaluation\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# exploratory data analysis\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# bayesian hyperparameter optimization\n",
    "from hyperopt import hp, Trials, fmin, tpe, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "from time import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Dataset\n",
    "https://www.kaggle.com/ninads/kernel3b5cdd2865/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_data = pd.read_csv(\"../data/bankcard.csv\", index_col=0)\n",
    "print(\"Shape of data: \", fraud_data.shape)\n",
    "fraud_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_data.isna().sum().sum() #Â check na"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fraud_data = fraud_data.drop([fraud_data.columns[0]], axis=1) # drop card number\n",
    "fraud_data['Date'] = pd.to_datetime(fraud_data['Date']) # convert to datetime\n",
    "fraud_data['CBK'] = fraud_data.apply(lambda x: 1 if x['CBK'] == 'Yes' else 0, axis=1) # binarize\n",
    "fraud_data['Hour'] = fraud_data.apply(lambda x: x.Date.hour, axis=1) # extract hour\n",
    "fraud_data['DayName'] = fraud_data.apply(lambda x: x.Date.weekday(), axis=1) # extract day name\n",
    "fraud_data['Day'] = fraud_data.apply(lambda x: x.Date.day, axis=1) # extract day\n",
    "fraud_data = fraud_data[['DayName', 'Day', 'Hour', 'Amount', 'CBK']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot settings\n",
    "sns.set(rc={'figure.figsize':(8, 5)})\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# get value counts and plot frequency of labels\n",
    "label_freq = fraud_data['CBK'].value_counts()\n",
    "ax = sns.barplot(x=label_freq.index, y=label_freq.values)\n",
    "\n",
    "# label values on top of bar\n",
    "for p in ax.patches:\n",
    "    perc = round((p.get_height()/len(fraud_data['CBK']))*100, 3)\n",
    "    ax.annotate(str(int(p.get_height())) + f' ({perc}%)', # label\n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), # location\n",
    "                ha = 'center', va = 'center', xytext = (0, 9), \\\n",
    "                textcoords = 'offset points')\n",
    "\n",
    "# label plot\n",
    "plt.title(\"Distribution of Labels\", size=14)\n",
    "plt.xlabel(\"Class Label\", size=12)\n",
    "plt.ylabel(\"Count\", size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Dataset\n",
    "- Train-Test split was done based on date. The optimal hyperparameters for each unsupervised algorithm is found on the train set, before testing on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['DayName', 'Hour', 'Amount']\n",
    "\n",
    "# extract train data\n",
    "X_train = fraud_data.loc[fraud_data['Day'] < 25][features].values\n",
    "y_train = fraud_data.loc[fraud_data['Day'] < 25].CBK\n",
    "\n",
    "X_test = fraud_data.loc[fraud_data['Day'] >= 25][features].values\n",
    "y_test = fraud_data.loc[fraud_data['Day'] >= 25].CBK\n",
    "\n",
    "print(f\"Train: {len(X_train)}\")\n",
    "print(f\"Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Besides the 3 methods compared in the paper (LOF, CBOF and ROCF), we also tested other outlier detection algorithms, namely K-Nearest Neighbors, Isolation Forest, and One-Class SVM.\n",
    "\n",
    "For these 6 methods, we conducted a hyperparameter tuning on the train set, to find the optimal parameters for each of the outlier detection algorithms for the credit card fraud dataset. We used Bayesian hyperparameter optimisation, which uses Bayes Theorem to direct the hyperparameter search in order to find the minimum or maximum of an objective function.\n",
    "\n",
    "The contamination factor was set at 0.00173 for all algorithms, which is the true contamination factor.\n",
    "\n",
    "In a later section of this notebook, we will show how the lack of knowledge on the contamination factor will affect the outlier detection results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Hyperparameter Optimisation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt(param_space, X, y, num_eval, classifier):  \n",
    "    '''\n",
    "    Function that performs Bayesian hyperparameter optimisation \n",
    "    to find the optimal parameters for the outlier detection algorithm.\n",
    "    \n",
    "    Inputs:\n",
    "        param_space (dict): A dictionary of the parameters and corresponding space to search.\n",
    "        X (array): Features of the dataset.\n",
    "        y (array): Labels of the dataset (0 = normal; 1 = anomaly).\n",
    "        \n",
    "        num_eval (int): Number of evaluation rounds.\n",
    "        classifier (pyOD Object): Outlier detection algorithm.\n",
    "        \n",
    "    Outputs:\n",
    "        trials\n",
    "        -min(loss) (float): Best in-sample F1 score.\n",
    "        best_param_values (dict): Dictionary of the best parameters for the classifier.\n",
    "    '''\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    def objective_function(params):\n",
    "        # initialise classifier\n",
    "        clf = classifier(**params)\n",
    "        # fit data\n",
    "        clf.fit(X)\n",
    "        # predict\n",
    "        try:\n",
    "            y_pred = clf.labels_\n",
    "        except: # ROCF algorithm\n",
    "            y_pred = clf.get_outliers()\n",
    "        # get F1 score\n",
    "        report = classification_report(y_true=y, y_pred=y_pred, output_dict=True)['1']\n",
    "        # objective is to maximize F1 i.e. minimize -F1\n",
    "        return {'status': STATUS_OK, 'loss': -report['f1-score'], 'precision': report['precision'], \n",
    "                'recall': report['recall']}\n",
    "    \n",
    "    trials = Trials()\n",
    "    \n",
    "    # minimise objective function\n",
    "    best_param = fmin(objective_function, param_space, algo=tpe.suggest, max_evals=num_eval, \n",
    "                      trials=trials, rstate= np.random.RandomState(1))\n",
    "    \n",
    "    loss = [x['result']['loss'] for x in trials.trials] \n",
    "    precision = [x['result']['precision'] for x in trials.trials] \n",
    "    recall = [x['result']['recall'] for x in trials.trials] \n",
    "    \n",
    "    best_ind = loss.index(min(loss))\n",
    "    \n",
    "    best_param_values = best_param\n",
    "    \n",
    "    return trials, -loss[best_ind], best_param_values, precision[best_ind], recall[best_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict to store hyperopt inputs for each algorithm\n",
    "hyperopt_inputs = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Outlier Factor (LOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameter search range\n",
    "LOF_param_hyperopt = {\n",
    "    'n_neighbors': scope.int(hp.quniform('n_neighbors', 50, 100, 1)),\n",
    "    'algorithm': hp.choice('algorithm', ['ball_tree', 'kd_tree', 'brute']),\n",
    "    'leaf_size': scope.int(hp.quniform('leaf_size', 10, 100, 1)),\n",
    "    'contamination': sum(y_train)/len(y_train), # set to actual outlier % \n",
    "}\n",
    "\n",
    "# num_eval proportional to number of combinations of parameter values for different models\n",
    "# num_eval = 3 ** num_params \n",
    "LOF_inputs = {'classifier': LOF, 'param_space': LOF_param_hyperopt, 'num_eval': 3**3}\n",
    "hyperopt_inputs['LOF'] = LOF_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_param_hyperopt = {\n",
    "    'contamination': sum(y_train)/len(y_train),\n",
    "    'n_neighbors': scope.int(hp.quniform('n_neighbors', 50, 90, 1)),\n",
    "    'method': hp.choice('method', ['largest', 'mean', 'median']),\n",
    "}\n",
    "\n",
    "KNN_inputs = {'classifier': KNN, 'param_space': KNN_param_hyperopt, 'num_eval': 3**2}\n",
    "hyperopt_inputs['KNN'] = KNN_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest (IForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IF_param_hyperopt = {\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 1, 80, 1)),\n",
    "    'max_samples': scope.int(hp.quniform('max_samples', 10, 50, 1)),    \n",
    "    'contamination': sum(y_train)/len(y_train),\n",
    "    'max_features': scope.int(hp.quniform('max_features', 1, 3, 1)),    \n",
    "}\n",
    "\n",
    "IF_inputs = {'classifier': IForest, 'param_space': IF_param_hyperopt, 'num_eval': 3**3}\n",
    "hyperopt_inputs['IForest'] = IF_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Class Support Vector Machine (OCSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OCSVM_param_hyperopt = {\n",
    "    'kernel': hp.choice('kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'nu': hp.uniform('nu', 0.4, 0.8),\n",
    "    'contamination': sum(y_train)/len(y_train),\n",
    "}\n",
    "\n",
    "OCSVM_inputs = {'classifier': OCSVM, 'param_space': OCSVM_param_hyperopt, 'num_eval': 3**2}\n",
    "hyperopt_inputs['OCSVM'] = OCSVM_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster-Based Outlier Factor (CBOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameter search range\n",
    "CBOF_param_hyperopt = {\n",
    "    'k': scope.int(hp.quniform('n_neighbors', 50, 100, 1)),\n",
    "    'lofub': hp.uniform('lofub', 0.5, 3.0),\n",
    "    'pct': hp.uniform('pct', 0.2, 0.8),\n",
    "    'contamination': sum(y_train)/len(y_train), # set to actual outlier % \n",
    "}\n",
    "\n",
    "CBOF_inputs = {'classifier': CBOF, 'param_space': CBOF_param_hyperopt, 'num_eval': 3**3}\n",
    "hyperopt_inputs['CBOF'] = CBOF_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Outlier Cluster Factor (ROCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROCF_param_hyperopt = {\n",
    "    'k': scope.int(hp.quniform('n_neighbors', 10, 30, 1)),\n",
    "}\n",
    "\n",
    "ROCF_inputs = {'classifier': ROCF, 'param_space': ROCF_param_hyperopt, 'num_eval': 3**1}\n",
    "hyperopt_inputs['ROCF'] = ROCF_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    fraud_results_tuned = pd.read_csv(\"fraud_results_tuned.csv\")\n",
    "\n",
    "except:\n",
    "    fraud_results_tuned = pd.DataFrame(columns=['algo', 'recall', 'precision', 'f1'])\n",
    "\n",
    "    for algo, algo_inputs in hyperopt_inputs.items():\n",
    "        # run hyperopt\n",
    "        algo_hyperopt = hyperopt(algo_inputs['param_space'], \\\n",
    "                                 X_train, y_train, \\\n",
    "                                 algo_inputs['num_eval'], algo_inputs['classifier'])\n",
    "        # retrieve best parameters\n",
    "        algo_opt = algo_hyperopt[2]\n",
    "        algo_opt['f1'] = algo_hyperopt[1] # add f1 score\n",
    "        algo_opt['precision'] = algo_hyperopt[3]\n",
    "        algo_opt['recall'] = algo_hyperopt[4]\n",
    "        algo_opt['algo'] = algo # add algo name\n",
    "        # add to results dataframe\n",
    "        fraud_results_tuned = fraud_results_tuned.append(algo_opt, ignore_index=True)\n",
    "\n",
    "    fraud_results_tuned.to_csv(\"fraud_results_tuned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_results_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_results_tuned[['algo', 'recall', 'precision', 'f1']].iloc[[0, 4, 5, 1, 2, 3], :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(columns=['algo', 'recall', 'precision', 'f1'])\n",
    "\n",
    "algos_dict = {'LOF': LOF, 'CBOF': CBOF, 'ROCF': ROCF, 'KNN': KNN, 'IForest': IForest, 'OCSVM': OCSVM}\n",
    "\n",
    "outlier_rate = sum(y_train) / len(y_train)\n",
    "\n",
    "# for each algorithm\n",
    "for algo_name, f in algos_dict.items():\n",
    "        \n",
    "    # get best parameters from tuning\n",
    "    algo_params = fraud_results_tuned.loc[fraud_results_tuned['algo'] == algo_name].reset_index().iloc[0]\n",
    "    outlier_rate_add = outlier_rate\n",
    "\n",
    "    # initialise classifier\n",
    "    if algo_name == 'LOF':\n",
    "        algo_lst = ['ball_tree', 'kd_tree', 'brute']\n",
    "        clf = LOF(algorithm=algo_lst[int(algo_params['algorithm'])], contamination=outlier_rate,\n",
    "                  leaf_size=algo_params['leaf_size'], n_neighbors=int(algo_params['n_neighbors']))\n",
    "\n",
    "    elif algo_name == 'KNN':\n",
    "        method_lst = ['largest', 'mean', 'median']\n",
    "        clf = KNN(n_neighbors=int(algo_params['n_neighbors']), contamination=outlier_rate, \\\n",
    "                  method=method_lst[int(algo_params['method'])])\n",
    "\n",
    "    elif algo_name == 'IForest':\n",
    "        clf = IForest(max_features=int(algo_params['max_features']), contamination=outlier_rate, \\\n",
    "                      max_samples=int(algo_params['max_samples']), n_estimators=int(algo_params['n_estimators']))\n",
    "\n",
    "    elif algo_name == 'CBOF':\n",
    "        clf = CBOF(k=int(algo_params['n_neighbors']), contamination=outlier_rate, \\\n",
    "                   lofub=algo_params['lofub'], pct=algo_params['pct'])\n",
    "\n",
    "    elif algo_name == 'OCSVM':\n",
    "        kernel_lst = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "        clf = OCSVM(kernel=kernel_lst[int(algo_params['kernel'])], \\\n",
    "                    nu=algo_params['nu'], contamination=outlier_rate)\n",
    "\n",
    "    elif algo_name == 'ROCF':\n",
    "        clf = ROCF(k=int(algo_params['n_neighbors']))\n",
    "            \n",
    "    # fit classifier on TEST data\n",
    "    clf.fit(X_test)\n",
    "\n",
    "    # retrieve predictions on TEST data\n",
    "    try:\n",
    "        y_pred = clf.get_outliers()\n",
    "    except:\n",
    "        y_pred = clf.labels_\n",
    "        \n",
    "    if algo_name == 'ROCF':\n",
    "        outlier_rate_add = clf.get_outlier_rate()\n",
    "\n",
    "    report = classification_report(y_true=y_test, y_pred=y_pred, output_dict=True)\n",
    "    f1 = report['1']['f1-score']\n",
    "    precision = report['1']['precision']\n",
    "    recall = report['1']['recall']\n",
    "    test_results = test_results.append({'algo': algo_name, 'outlier_rate': outlier_rate_add, 'recall': recall, \\\n",
    "                                        'precision': precision, 'f1': f1}, \\\n",
    "                                         ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results[['algo', 'outlier_rate', 'recall', 'precision', 'f1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of \"Top-n\" parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outlier = sum(y_train) / len(y_train)\n",
    "test_outlier = sum(y_test) / len(y_test)\n",
    "\n",
    "top_n_results = pd.DataFrame(columns=['algo', 'outlier_rate', 'recall', 'precision', 'f1'])\n",
    "\n",
    "outlier_rate_lst = [train_outlier, test_outlier, test_outlier + 0.02]\n",
    "algos_dict = {'LOF': LOF, 'CBOF': CBOF, 'ROCF': ROCF, 'KNN': KNN, 'IForest': IForest, 'OCSVM': OCSVM}\n",
    "\n",
    "# for each algorithm\n",
    "for algo_name, f in algos_dict.items():\n",
    "    \n",
    "    # for each outlier rate\n",
    "    for outlier_rate in outlier_rate_lst:\n",
    "        outlier_rate_add = outlier_rate\n",
    "        \n",
    "        # get best parameters from tuning\n",
    "        algo_params = fraud_results_tuned.loc[fraud_results_tuned['algo'] == algo_name].reset_index().iloc[0]\n",
    "        \n",
    "        # initialise classifier\n",
    "        if algo_name == 'LOF':\n",
    "            algo_lst = ['ball_tree', 'kd_tree', 'brute']\n",
    "            clf = LOF(algorithm=algo_lst[int(algo_params['algorithm'])], contamination=outlier_rate,\n",
    "                      leaf_size=algo_params['leaf_size'], n_neighbors=int(algo_params['n_neighbors']))\n",
    "            \n",
    "        elif algo_name == 'KNN':\n",
    "            method_lst = ['largest', 'mean', 'median']\n",
    "            clf = KNN(n_neighbors=int(algo_params['n_neighbors']), contamination=outlier_rate, \\\n",
    "                      method=method_lst[int(algo_params['method'])])\n",
    "            \n",
    "        elif algo_name == 'IForest':\n",
    "            clf = IForest(max_features=int(algo_params['max_features']), contamination=outlier_rate, \\\n",
    "                          max_samples=int(algo_params['max_samples']), n_estimators=int(algo_params['n_estimators']))\n",
    "            \n",
    "        elif algo_name == 'CBOF':\n",
    "            clf = CBOF(k=int(algo_params['n_neighbors']), contamination=outlier_rate, \\\n",
    "                       lofub=algo_params['lofub'], pct=algo_params['pct'])\n",
    "            \n",
    "        elif algo_name == 'OCSVM':\n",
    "            kernel_lst = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "            clf = OCSVM(kernel=kernel_lst[int(algo_params['kernel'])], \\\n",
    "                        nu=algo_params['nu'], contamination=outlier_rate)\n",
    "        \n",
    "        elif algo_name == 'ROCF':\n",
    "            if outlier_rate != test_outlier:\n",
    "                continue\n",
    "            else:\n",
    "                clf = ROCF(k=int(algo_params['n_neighbors']))\n",
    "            \n",
    "        # fit classifier on data\n",
    "        clf.fit(X_test)\n",
    "        \n",
    "        # retrieve predictions\n",
    "        try:\n",
    "            y_pred = clf.get_outliers()\n",
    "        except:\n",
    "            y_pred = clf.labels_\n",
    "        \n",
    "        \n",
    "        if algo_name == 'ROCF':\n",
    "            outlier_rate_add = clf.get_outlier_rate()\n",
    "            \n",
    "        report = classification_report(y_true=y_test, y_pred=y_pred, output_dict=True)\n",
    "        f1 = report['1']['f1-score']\n",
    "        precision = report['1']['precision']\n",
    "        recall = report['1']['recall']\n",
    "        top_n_results = top_n_results.append({'algo': algo_name, 'outlier_rate': outlier_rate_add, 'recall': recall, \\\n",
    "                                              'precision': precision, 'f1': f1}, \\\n",
    "                                             ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of ROCF 0.1 Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocf_algo = ROCF(k=9)\n",
    "rocf_algo.fit(X_test)\n",
    "max(rocf_algo.get_rocfs()) # print max rocf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
